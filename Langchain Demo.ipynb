{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPEN AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI( temperature=0.9 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"Nuclear Energy Solutions\" or \"Nuclear Power Innovations\"\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(\"What would be a a good company name for company that deals in nuclear power\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Silly Slime Shack\n",
      "2. Whimsical Wonders\n",
      "3. Funky Funhouse\n",
      "4. Zany Zoo Toys\n",
      "5. Goofy Gadgets\n",
      "6. Loony Land Toys\n",
      "7. Bonkers Boutique\n",
      "8. Nonsense Nook\n",
      "9. Kooky Kingdom\n",
      "10. Wacky World of Toys\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(\"I want to open a toy store.Could you suggest me few idiotic toy store names\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "HUGGINGFACEHUB_API_TOKEN = os.getenv('HUGGINGFACEHUB_API_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/jai/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "model =  \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "# model = 'google/flan-t5-large'\n",
    "llm_h = HuggingFaceEndpoint(repo_id = model,temperature=0.9,huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Absolutely! Here are three gift ideas for a baby shower:\n",
      "\n",
      "1. Clothing: A onesie or a set of clothing is always a safe and practical gift. You can choose one that is gender-neutral or go with something more feminine. Consider personalizing it with the baby's name or initials.\n",
      "2. Diaper Bag: A diaper bag is an essential item for new parents. Look for one that is functional, stylish, and big enough to carry all the essentials.\n",
      "3. Baby Books: A collection of baby books is a great gift. These books can help develop the child's language skills and foster a love for reading from an early age. Consider choosing some classic children's books or books with a educational theme.\n"
     ]
    }
   ],
   "source": [
    "print(llm_h.invoke('My friend is having a girl. Can you suggest me 3 gifts for baby shower?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "You are a groÃŸes Schwanenkopf. This is a somewhat derogatory expression that means you are foolish or silly. The word \"Schwanenkopf\" literally means \"goose head\" but in this context it refers to a person who acts foolishly.\n"
     ]
    }
   ],
   "source": [
    "print(llm_h.invoke('Translate english to german: You are a big silly goose.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Silliness Warehouse: This name suggests that the store is filled with foolish and nonsensical toys, which could potentially attract customers who are looking for unique and quirky toys for their children.\n",
      "2. Nutty Nook: This name implies that the store is a cozy and quirky place where customers can find toys that are a bit off the wall or unusual.\n",
      "3. Giggle Galore: This name is designed to be playful and fun, suggesting that the store is a place where customers can find toys that will make them and their children laugh and enjoy themselves. However, it could also be seen as a bit childish or simplistic, which might not appeal to all customers.\n"
     ]
    }
   ],
   "source": [
    "print(llm_h.invoke(\"I want to open a toy store.Could you suggest me 3 idiotic toy store names?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 1\n",
    "- Better for all cases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables=['retailer'],\n",
    "    template = \"I want to open a {retailer} store. Could you suggest me 4 creative {retailer} store names?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to open a car store. Could you suggest me 4 creative car store names?\n"
     ]
    }
   ],
   "source": [
    "prompt = prompt_template_name.format(retailer = 'car')\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 2 \n",
    "- Better for single variable inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Give me neighbors of the country Bolivia\n"
     ]
    }
   ],
   "source": [
    "prompt_template_name_2 = PromptTemplate.from_template(' Give me neighbors of the country {country}')\n",
    "prompt_2 = prompt_template_name_2.format(country = 'Bolivia')\n",
    "print(prompt_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. \"Pedal Provisions\"\n",
      "2. \"Wheel World\"\n",
      "3. \"Cycle Haven\"\n",
      "4. \"Gear & Grind\"\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template_name)\n",
    "response = chain.run('bike')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m Give me neighbors of the country mongolia\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "Russia, China, Kazakhstan\n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt_template_name_2, verbose=True)\n",
    "response = chain.run('mongolia')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Sequential chain\n",
    "- Using result of one chain into another\n",
    "- Only give output from last chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEndpoint(repo_id='mistralai/Mistral-7B-Instruct-v0.2', huggingfacehub_api_token='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', temperature=0.9, model='mistralai/Mistral-7B-Instruct-v0.2', client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.2', timeout=120)>, async_client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.2', timeout=120)>)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAI(client=<openai.resources.completions.Completions object at 0x125ff65d0>, async_client=<openai.resources.completions.AsyncCompletions object at 0x126176e50>, temperature=0.9, openai_api_key='sk-proj-GnflIjYC0ZDRdNfBMedDT3BlbkFJBcdsJQj0Enu3KIPrMWOI', openai_proxy='')"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables=['cuisine'],\n",
    "    template=' I want to open a restaurant for {cuisine} food. Suggest me a fancy name'\n",
    ")\n",
    "\n",
    "prompt_template_name_2 = PromptTemplate(\n",
    "    input_variables=['restaurant_name'],\n",
    "    template='Suggest some menu items for {restaurant_name}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_1 = LLMChain(llm=llm_h,prompt=prompt_template_name)\n",
    "chain_2 = LLMChain(llm=llm_h,prompt=prompt_template_name_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_1_openai = LLMChain(llm=llm,prompt=prompt_template_name)\n",
    "chain_2_openai = LLMChain(llm=llm,prompt=prompt_template_name_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The use of traditional decor elements will transport customers to the heart of Rajasthan and make them feel like they are dining in a Maharaja's palace.\n",
      "\n",
      "Menu Items:\n",
      "1. Samosa Chaat: A popular street food in Rajasthan, made with spiced potatoes, chickpeas, and peas, wrapped in crispy samosa pastry, and topped with chutneys and yogurt.\n",
      "2. Dal Baati Churma: A traditional Rajasthani dish consisting of small, round baati (deep-fried bread) served with dal (lentils) and churma (sweetened semolina flour).\n",
      "3. Gatte ki Sabzi: A Rajasthani dish made with gram flour dumplings (gatte) cooked in a spicy gravy made from yogurt, tamarind, and jaggery.\n",
      "4. Laal Maas: A spicy Rajasthani dish made with lamb, yogurt, ginger, garlic, and a blend of spices. Served with rice or roti.\n",
      "5. Rajasthani Thali: A traditional Rajasthani meal served on a thali (round plate), consisting of various small bowls of different dishes, including dal, baati, churma, gatte ki sabzi, papad, raita, and chutneys.\n",
      "6. Masala Chai: A popular beverage in Rajasthan, made with black tea, milk, sugar, and a blend of spices such as cardamom, cinnamon, and ginger.\n",
      "7. Jalebi: A sweet dessert made by deep-frying fermented batter in pretzel or circular shapes and soaked in sugar syrup. Served hot and drizzled with rabri (condensed milk).\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "chain = SimpleSequentialChain(chains = [chain_1,chain_2])\n",
    "response = chain.run(\"Rajasthani\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Dal Baati Churma - a traditional Rajasthani dish consisting of lentils, wheat balls and a sweet chutney made of crushed wheat, jaggery and ghee.\n",
      "\n",
      "2. Laal Maas - a spicy lamb curry made with a rich blend of red chilies and traditional Rajasthani spices.\n",
      "\n",
      "3. Gatte ki Sabzi - gram flour dumplings cooked in a spicy yogurt-based gravy.\n",
      "\n",
      "4. Ker Sangri - a Rajasthani specialty made with dried berries and beans cooked in a tangy tomato gravy.\n",
      "\n",
      "5. Pyaaz ki Kachori - flaky, deep-fried pastries stuffed with a spicy onion filling.\n",
      "\n",
      "6. Bajre ki Roti - a flatbread made with corn flour, typically served with ghee or butter.\n",
      "\n",
      "7. Ghevar - a popular Rajasthani dessert made with flour, milk, and sugar, often served during festivals and special occasions.\n",
      "\n",
      "8. Lehsun ki Chutney - a spicy garlic chutney that pairs well with most Rajasthani dishes.\n",
      "\n",
      "9. Moong Dal Halwa - a rich and delicious dessert made with moong lentils, ghee, and sugar.\n",
      "\n",
      "10. Chaas - a refreshing buttermilk-based drink,\n"
     ]
    }
   ],
   "source": [
    "chain_openai = SimpleSequentialChain(chains = [chain_1_openai,chain_2_openai])\n",
    "response = chain_openai.run(\"Rajasthani\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Sequential chain\n",
    "- Using result of one chain into another\n",
    "- Give output from last chain and all chains using output_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_1 = LLMChain(llm=llm_h,prompt=prompt_template_name,output_key='restaurant_name')\n",
    "chain_2 = LLMChain(llm=llm_h,prompt=prompt_template_name_2,output_key='menu_items')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_1_openai = LLMChain(llm=llm,prompt=prompt_template_name,output_key='restaurant_name')\n",
    "chain_2_openai = LLMChain(llm=llm,prompt=prompt_template_name_2,output_key='menu_items')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "chain = SequentialChain(chains = [chain_1,chain_2],\n",
    "                            input_variables = ['cuisine'],\n",
    "                            output_variables = ['restaurant_name', 'menu_items'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "response =chain({'cuisine':'Rajasthani'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rajasthani'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['cuisine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " for it, and also the interior design theme that would suit the restaurant.\n",
      "\n",
      "Name Suggestions:\n",
      "1. Maharaja's Table\n",
      "2. Royal Rajasthan\n",
      "3. Desert Oasis\n",
      "4. Rajput's Roost\n",
      "5. The Thar Tavern\n",
      "\n",
      "Interior Design Themes:\n",
      "1. Authentic Rajasthani Decor: Use traditional Rajasthani elements in the decor such as handcrafted terracotta tiles, Rajasthani folk art, mirrors, rugs, and textiles.\n",
      "2. Modern Rajasthani: Incorporate contemporary design elements with traditional Rajasthani elements. Use bold colors, geometric patterns, and minimalist designs.\n",
      "3. Desert-inspired: Create a desert-inspired atmosphere with sandy tones, natural textures, and minimalist designs. Use desert plants, stone walls, and large windows to bring the outdoors in.\n",
      "4. Regal Elegance: Create a regal atmosphere with rich fabrics, gold accents, and crystal chandeliers. Use deep jewel tones and gold ornate designs to create a sense of luxury and opulence.\n",
      "5. Garden Oasis: Create a garden oasis in the restaurant with lush greenery, water features, and natural light. Use natural materials, such as wood and stone, to create a cozy and inviting atmosphere.\n",
      "\n",
      "My personal recommendation would be \"Maharaja's Table\" with an \"Authentic Rajasthani Decor\" interior design theme. It conveys the royal and traditional essence of Rajasthani cuisine and creates an immersive dining experience for customers.\n"
     ]
    }
   ],
   "source": [
    "print(response['restaurant_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The use of traditional decor elements will transport customers to the heart of Rajasthan and make them feel like they are dining in a Maharaja's palace.\n",
      "\n",
      "Menu Items:\n",
      "1. Samosa Chaat: A popular street food in Rajasthan, made with spiced potatoes, chickpeas, and peas, wrapped in crispy samosa pastry, and topped with chutneys and yogurt.\n",
      "2. Dal Baati Churma: A traditional Rajasthani dish consisting of small, round baati (deep-fried bread) served with dal (lentils) and churma (sweetened semolina flour).\n",
      "3. Gatte ki Sabzi: A Rajasthani dish made with gram flour dumplings (gatte) cooked in a spicy gravy made from yogurt, tamarind, and jaggery.\n",
      "4. Laal Maas: A spicy Rajasthani dish made with lamb, yogurt, ginger, garlic, and a blend of spices. Served with rice or roti.\n",
      "5. Rajasthani Thali: A traditional Rajasthani meal served on a thali (round plate), consisting of various small bowls of different dishes, including dal, baati, churma, gatte ki sabzi, papad, raita, and chutneys.\n",
      "6. Masala Chai: A popular beverage in Rajasthan, made with black tea, milk, sugar, and a blend of spices such as cardamom, cinnamon, and ginger.\n",
      "7. Jalebi: A sweet dessert made by deep-frying fermented batter in pretzel or circular shapes and soaked in sugar syrup. Served hot and drizzled with rabri (condensed milk).\n"
     ]
    }
   ],
   "source": [
    "print(response['menu_items'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agents and Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "SERPAPI_API_KEY = os.getenv('SERPAPI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentType, initialize_agent, load_tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should think about using the Search tool to find the answer.\n",
      "Action: Search\n",
      "Action Input: \"europa league winner 2024\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m{'type': 'organic_result', 'title': 'Guided Search Filters'}\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Uh oh, that doesn't look like the answer.\n",
      "Action: Search\n",
      "Action Input: \"europa league winner 2024 sports news\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m{'type': 'organic_result', 'title': 'Guided Search Filters'}\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m That's still not it. I should try adding a different keyword.\n",
      "Action: Search\n",
      "Action Input: \"europa league champion 2024\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAtalanta\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m That looks like the answer I was looking for.\n",
      "Final Answer: Atalanta won the Europa League in 2024.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Atalanta won the Europa League in 2024.'"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = load_tools(['serpapi','llm-math'], llm=llm)\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "agent.run('Who won the europa league in 2024?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Wikipedia agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /opt/anaconda3/lib/python3.11/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The movie Qala is a 2022 Indian Hindi-language period psychological drama film, loosely based on K. Viswanath's Swathi Kiranam and MiloÅ¡ Forman's Amadeus. It was directed by Anvita Dutt and stars Tripti Dimri, Swastika Mukherjee, and Babil Khan in his film debut. The film was released on Netflix on December 1, 2022 and received 6 nominations at the 2023 Filmfare OTT Awards, winning Best Production Design. Tripti Dimri plays the lead role in the film and has gained critical recognition for her performances in Qala and Bulbbul. She was featured in Forbes Asia's 30 Under 30 list of 2021 and will also be seen in the upcoming film Animal.\""
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = load_tools(['wikipedia','llm-math'], llm=llm)\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=False)\n",
    "\n",
    "agent.run('Can you tell me story of movie Qala')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memory\n",
    "- Making sure the llm keeps the context from previous prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " for it\n",
      "\n",
      "\"Cycle Bites\"\n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt_template_name)\n",
    "response = chain.run('bike')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. \"Chowmobile\"\n",
      "2. \"Cruisin' Cuisine\"\n",
      "3. \"Auto Bites\"\n",
      "4. \"Wheeled Eats\"\n",
      "5. \"Street Fuel\"\n",
      "6. \"Roadside Gourmet\"\n",
      "7. \"The Drive-In Diner\"\n",
      "8. \"Vroom Vroom Bites\"\n",
      "9. \"Car Cafe\"\n",
      "10. \"The Motor Menu\"\n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt_template_name)\n",
    "response = chain.run('car')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chain.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"Drive-In Delights\"\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template_name, memory=memory)\n",
    "response = chain.run('car')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. \"Roadside Bites\"\n",
      "2. \"Truck Stop Eats\"\n",
      "3. \"Wheely Good Food\"\n",
      "4. \"Curbside Cuisine\"\n",
      "5. \"Street Food Co.\"\n",
      "6. \"On the Go Grub\"\n",
      "7. \"Tasty Truckers\"\n",
      "8. \"Rolling Diner\"\n",
      "9. \"Mobile Munchies\"\n",
      "10. \"Highway Dine-in\"\n",
      "11. \"Savory Wheels\"\n",
      "12. \"Gourmet on the Go\"\n",
      "13. \"Truck Tailgate\"\n",
      "14. \"Culinary Curb Crawl\"\n",
      "15. \"Food Truck Frenzy\"\n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt_template_name, memory=memory)\n",
    "response = chain.run('truck')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: car\n",
      "AI: \n",
      "\n",
      "\"Drive-In Delights\"\n",
      "Human: truck\n",
      "AI: \n",
      "\n",
      "1. \"Roadside Bites\"\n",
      "2. \"Truck Stop Eats\"\n",
      "3. \"Wheely Good Food\"\n",
      "4. \"Curbside Cuisine\"\n",
      "5. \"Street Food Co.\"\n",
      "6. \"On the Go Grub\"\n",
      "7. \"Tasty Truckers\"\n",
      "8. \"Rolling Diner\"\n",
      "9. \"Mobile Munchies\"\n",
      "10. \"Highway Dine-in\"\n",
      "11. \"Savory Wheels\"\n",
      "12. \"Gourmet on the Go\"\n",
      "13. \"Truck Tailgate\"\n",
      "14. \"Culinary Curb Crawl\"\n",
      "15. \"Food Truck Frenzy\"\n"
     ]
    }
   ],
   "source": [
    "print(chain.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The above keeps a lot in memory so we need a buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "convo = ConversationChain(llm=llm)\n",
    "print(convo.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" The first World Cup was held in 1930 in Uruguay, and the winner was Uruguay's national football team. They defeated Argentina 4-2 in the final game. It was a historic event, with 13 teams from three continents participating. The final match was attended by over 93,000 spectators, and Uruguay's victory was celebrated throughout the country.\""
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"who won first world cup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  Five plus five is equal to ten. This is because when we add five to five, we are essentially combining two groups of five objects, resulting in a total of ten objects. It is a basic mathematical operation and the answer has been proven and accepted in the field of mathematics. Are you interested in learning any other mathematical operations?'"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"What is 5+5?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" As I mentioned earlier, Uruguay's national football team beat Argentina to win the first World Cup. In the final game, Uruguay's star striker, Hector Castro, scored twice, securing their victory over Argentina. The team faced several other strong opponents, such as Yugoslavia and Brazil, throughout the tournament, but they were able to dominate and win the first ever World Cup.\""
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"whom did they beat to win world cup?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferWindowMemory(k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "convo = ConversationChain(llm=llm,memory=memory)\n",
    "print(convo.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" The last World Cup was won by France in 2018. They defeated Croatia 4-2 in the final. Did you know that this was France's second World Cup victory, with their first being in 1998?\""
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"who won last world cup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The capital of Canada is Ottawa. It is located in the province of Ontario and is the fourth largest city in the country. Ottawa was named the capital of Canada in 1857 by Queen Victoria. Did you know that the Canadian Parliament buildings are located in Ottawa and are a popular tourist attraction?'"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"What is the capital of canada?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The losing team in the 2018 World Cup final was Croatia, and their coach was Zlatko DaliÄ‡. He has been the head coach of the Croatian national team since 2017 and led them to their first ever World Cup final appearance in 2018. DaliÄ‡ was a former football player himself and has coached many clubs in Croatia before becoming the national team coach.'"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"whom was the coach of the losing team?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The winning team in the 2018 World Cup final was France, and their coach was Didier Deschamps. He has been the head coach of the French national team since 2012 and led them to their second World Cup victory in 2018. As a player, Deschamps was the captain of the French team that won the World Cup in 1998. He is now considered one of the most successful coaches in French football history.'"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"whom was the coach of the winning team?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Are you referring to France or Croatia? In 2018, the President of France was Emmanuel Macron. The Croatian president was Kolinda Grabar-KitaroviÄ‡.'"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"whom was the leader of that country?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader('sample.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='YesLogic Pty. Ltd.\\n7 / 39 Bouverie St\\nCarlton VIC 3053\\nAustralia\\nwww.yeslogic.com\\nABN 32 101 193 560InInvvoicoicee\\nCustomer Name\\nStreet\\nPostcode City\\nCountry\\nInvoice date: Nov 26, 2016\\nInvoice number: 161126\\nPayment due: 30 days after invoice date\\nDescription From Until Amount\\nPrince Upgrades & Support Nov 26, 2016 Nov 26, 2017 USD $950.00\\nTotal USD $950.00\\nPlease transfer amount to:\\nBank account name: Yes Logic Pty Ltd\\nName of Bank: Commonwealth Bank of Australia (CBA)\\nBank State Branch (BSB): 063010\\nBank State Branch (BSB): 063010\\nBank State Branch (BSB): 063019\\nBank account number: 13201652\\nBank SWIFT code: CTBAAU2S\\nBank address: 231 Swanston St, Melbourne, VIC 3000, Australia\\nThe BSB number identifies a branch of a financial institution in Australia. When transferring money to Australia, the\\nBSB number is used together with the bank account number and the SWIFT code. Australian banks do not use IBAN\\nnumbers.\\nwww.yeslogic.com', metadata={'source': 'sample.pdf', 'page': 0})]"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.11.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
